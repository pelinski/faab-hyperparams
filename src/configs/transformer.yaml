project: "faab_autoencoder_transformer"
# dataset parameters
pickle_path: "src/dataset/data/0117/0117_processed_1024.pkl"
weights_path: "src/dataset/data/0117/weights.pkl"
pred: False
seq_len: 1024
comp_seq_len: 1024
batch_size: 64

# common model parameters
feat_len: 8 # feature size
comp_feat_len: 4
ff_size_features: 12
ff_size_time: 2048
num_layers: 4

# model parameters transformer
model: "transformer"
num_heads: 2
pe_scale_factor: 1
mask: False

# training parameters
max_grad_norm: 0
dropout: 0
epochs: 150
optimizer: "rmsprop"
criterion: "weighted_mse"
learning_rate: 0.0001
scheduler_step_size: 0
scheduler_gamma: 0.1
plotter_samples: 5
