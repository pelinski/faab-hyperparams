project: "faab_autoencoder_transformer"
# dataset parameters
pickle_path: "src/dataset/data/0117/0117_processed_1024.pkl"
weights_path: "src/dataset/data/0117/weights.pkl"
pred: False
seq_len: 1024
batch_size: 64

# common model parameters
feat_in_size: 8 # feature size
d_model: 4
ff_size: 12
num_layers: 4

# model parameters transformer
model: "transformer"
num_heads: 2
pe_scale_factor: 1
mask: False

# training parameters
max_grad_norm: 0
dropout: 0
epochs: 150
optimizer: "rmsprop"
criterion: "weighted_mse"
learning_rate: 0.0001
scheduler_step_size: 0
scheduler_gamma: 0.1
plotter_samples: 5
